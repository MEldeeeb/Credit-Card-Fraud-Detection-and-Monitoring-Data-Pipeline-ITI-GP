{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c82d4b5-3832-42e8-ae12-280e8cf54033",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, datediff, current_date\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "import math\n",
    "from hdfs import InsecureClient\n",
    "import pandas as pd\n",
    "\n",
    "# Start Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FraudDetectionSparkML\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load data\n",
    "client = InsecureClient('http://hadoop-namenode:9870', user='root')\n",
    "with client.read('/data') as reader:\n",
    "    df = pd.read_csv(reader)\n",
    "    \n",
    "# Haversine formula as a Spark UDF\n",
    "def haversine(lat, lon, merch_lat, merch_lon):\n",
    "    R = 6371.0\n",
    "    lat1 = math.radians(lat)\n",
    "    lon1 = math.radians(lon)\n",
    "    lat2 = math.radians(merch_lat)\n",
    "    lon2 = math.radians(merch_lon)\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = math.sin(dlat / 2) ** 2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2) ** 2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    return R * c\n",
    "\n",
    "# Spark ML stages\n",
    "\n",
    "\n",
    "# Save the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8723dc8-4f5b-4b48-9168-03fc3789c84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "haversine_udf = udf(haversine, DoubleType())\n",
    "\n",
    "# Feature engineering\n",
    "df = df.withColumn(\"dob\", col(\"dob\").cast(\"date\"))\n",
    "df = df.withColumn(\"age\", (datediff(current_date(), col(\"dob\")) / 365.25).cast(IntegerType()))\n",
    "df = df.withColumn(\"distance\", haversine_udf(col(\"lat\"), col(\"long\"), col(\"merch_lat\"), col(\"merch_long\")))\n",
    "\n",
    "# Drop unused columns\n",
    "drop_cols = [\"Unnamed: 0\", \"trans_date_trans_time\", \"trans_num\", \"dob\", \"unix_time\",\n",
    "             \"lat\", \"long\", \"merch_lat\", \"merch_long\", \"first\", \"last\"]\n",
    "df = df.drop(*drop_cols)\n",
    "\n",
    "# Undersampling (randomly reduce majority class)\n",
    "fraud_df = df.filter(col(\"is_fraud\") == 1)\n",
    "nonfraud_df = df.filter(col(\"is_fraud\") == 0).sample(fraction=fraud_df.count() / df.filter(col(\"is_fraud\") == 0).count(), seed=42)\n",
    "df_balanced = fraud_df.union(nonfraud_df)\n",
    "\n",
    "# Categorical & numeric columns\n",
    "categorical_cols = [field.name for field in df_balanced.schema.fields if str(field.dataType) == \"StringType\"]\n",
    "numeric_cols = [field.name for field in df_balanced.schema.fields if str(field.dataType) != \"StringType\" and field.name != \"is_fraud\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5170ef88-f466-4e26-a82b-5d106b23fad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "indexers = [StringIndexer(inputCol=colname, outputCol=colname + \"_index\", handleInvalid=\"keep\") for colname in categorical_cols]\n",
    "encoders = [OneHotEncoder(inputCol=colname + \"_index\", outputCol=colname + \"_ohe\") for colname in categorical_cols]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[col + \"_ohe\" for col in categorical_cols] + numeric_cols, outputCol=\"features\")\n",
    "\n",
    "# Classifier (can replace with xgboost4j-spark if needed)\n",
    "classifier = GBTClassifier(labelCol=\"is_fraud\", featuresCol=\"features\", maxIter=50)\n",
    "\n",
    "# Build pipeline\n",
    "pipeline = Pipeline(stages=indexers + encoders + [assembler, classifier])\n",
    "\n",
    "# Train/test split\n",
    "train_df, test_df = df_balanced.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Train model\n",
    "model = pipeline.fit(train_df)\n",
    "\n",
    "# Evaluate\n",
    "predictions = model.transform(test_df)\n",
    "predictions.select(\"is_fraud\", \"prediction\", \"probability\").show(5, truncate=False)\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"is_fraud\", metricName=\"accuracy\")\n",
    "print(\"Test Accuracy:\", evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372aad92-885b-4b97-8954-c170fc1e5d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"fraud_detection_sparkml_model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
